{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: blue\"> SUMMARIZE DIALOGUE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets==2.17.0 in /home/rfjd/.local/lib/python3.8/site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets==2.17.0) (2.22.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/rfjd/.local/lib/python3.8/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/rfjd/.local/lib/python3.8/site-packages (from datasets==2.17.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets==2.17.0) (5.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rfjd/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/rfjd/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rfjd/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rfjd/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/rfjd/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/rfjd/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rfjd/.local/lib/python3.8/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/rfjd/.local/lib/python3.8/site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rfjd/.local/lib/python3.8/site-packages (from pandas->datasets==2.17.0) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.17.0) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets==2.17.0) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /home/rfjd/.local/lib/python3.8/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets==2.17.0 # installs the datasets library from Hugging Face\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet\n",
    "%pip install transformers==4.27.2 --quiet # installs the transformers library from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports from Hugging Face libraries `datasets` and `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM # loads a pre-trained sequence-to-sequence model\n",
    "from transformers import AutoTokenizer # loads the appropriate tokenizer for any given model\n",
    "from transformers import GenerationConfig # allows you to configure generation parameters for text generation tasks, such as max length, temperature, top-k sampling, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green\"> Dataset & Model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f134e68088c4d03bd2c8e4ffc76809c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839218ece68945d98f02c46d52e2f19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c91f3c9f5f940bf8e4fb32851a2f7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02be9f16c554ac6a5fbb015771db3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc7cf3291874522b90e0b9c5b8a6e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096474e27da64665b1c0c20aa8cf3c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3f34ce62714a5f8cfb6c454395b143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name) # DatasetDict object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(dataset): <class 'datasets.dataset_dict.DatasetDict'>\n",
      "len(dataset) = 3\n",
      "dataset.keys(): dict_keys(['train', 'validation', 'test'])\n",
      "type(dataset['test']): <class 'datasets.arrow_dataset.Dataset'>\n",
      "testdata[0].keys(): dict_keys(['id', 'dialogue', 'summary', 'topic'])\n",
      "number of examples in train data      : 12460\n",
      "number of examples in validation data : 500\n",
      "number of examples in test data       : 1500\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(dataset): {type(dataset)}\")\n",
    "print(f\"len(dataset) = {len(dataset)}\")\n",
    "print(f\"dataset.keys(): {dataset.keys()}\")\n",
    "print(f\"type(dataset['test']): {type(dataset['test'])}\")\n",
    "print(f\"testdata[0].keys(): {dataset['test'][0].keys()}\")\n",
    "print(f\"number of examples in train data      : {len(dataset['train'])}\")\n",
    "print(f\"number of examples in validation data : {len(dataset['validation'])}\")\n",
    "print(f\"number of examples in test data       : {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- `dataset` is **DatasetDict** dictionary with keys *train*, *validation*, and *test*.\n",
    "- Each member of the dataset, e.g., `dataset['test']` includes `examples` that can be accessed by indexing: `dataset['test'][index]`\n",
    "- Each `example` is a dictionary with keys *id*, *dialogue*, *summary*, and *topic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "Example 0\n",
      "\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "\n",
      "BASELINE HUMAN SUMMARY:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "________________________________________\n",
      "Example 1\n",
      "\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What do you think of this one? \n",
      "#Person2#: Eh, so so. \n",
      "#Person1#: And this one? Too flashy? \n",
      "#Person2#: Nah, not too flashy. \n",
      "#Person1#: Uhg! And this sweater from my aunt? Isn't it hideous? \n",
      "#Person2#: I guess. \n",
      "#Person1#: Are you even listening? I'm trying to have a conversation with you. \n",
      "#Person2#: And I'm trying to watch the game, but you're yapping on about your new clothes! \n",
      "#Person1#: Well I have to decide which gifts to keep and which to exchange for better ones when I go to the Boxing Day sales this afternoon! \n",
      "#Person2#: Well could you do me the favor of making this quick? It's the third quarter and you've been blabbering on since the first! \n",
      "#Person1#: Oh, your precious game. You watch the same game every year, and each year your beloved hometown team loses by at least three goals! \n",
      "#Person2#: Oh no you didn't. You didn't just insult the Sals-bury Seals, did you? Why don't you just. just go and return all of those stupid clothes and not come back until the sales are over? \n",
      "#Person1#: I might just! Enjoy your stupid game! \n",
      "\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is showing the new clothes to #Person2#, but #Person2# is busy watching the games. They quarrel and get angry.\n"
     ]
    }
   ],
   "source": [
    "testdata = dataset['test']\n",
    "example_indices = [1, 123]\n",
    "\n",
    "hbar = '_'*40\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(hbar)\n",
    "    print(f\"Example {i}\")\n",
    "    print('\\nINPUT DIALOGUE:')\n",
    "    print(testdata[index]['dialogue'])\n",
    "    print('\\nBASELINE HUMAN SUMMARY:')\n",
    "    print(testdata[index]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model: FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iAYlS40Z3l-v",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rfjd/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465cdcc698ff4cd48f8f2b1cc977565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rfjd/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5272eeebda4c7ba9d39c5ee49f26cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109bce55c8214e69a3e627cb9d15e018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # creates an instance of AutoModelForSeq2SeqLM class with .from_pretrained() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sPqQA3TT3l_I",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a708fa0b2ba45fb973ee7bad2ec268f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af54294cb2034ac1b674f00aa359a86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43da1be66480457faa046d92d5bd3f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550652d152d5424eb2ec07f566e9ff43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) # tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Fast Tokenizer: Some models support a fast tokenizer implementation built on the Rust programming language, which is much faster than the standard Python implementation. Fast tokenizers provide significant performance improvements when tokenizing large amounts of text.\n",
    "- Standard Tokenizer: The standard tokenizer implementation is written in Python and is slower compared to the fast version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE: tensor([  571,    19,    34,   352,   377, 18965,  1271,  2644,    58,     1])\n",
      "SENTENCE:         How is it going Fakhreddin?\n",
      "DECODED SENTENCE: How is it going Fakhreddin?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"How is it going Fakhreddin?\"\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt') # return tensors in PyTorch ('pt') format\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"ENCODED SENTENCE: {sentence_encoded['input_ids'][0]}\")\n",
    "print(f\"SENTENCE:         {sentence}\")\n",
    "print(f\"DECODED SENTENCE: {sentence_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Other options for `return_tensors` are `'tf'` and `'np'` for tensorflow and numpy, respectively.\n",
    "- `sentence_encoded` is a dictionary-like object with key 'input_ids'\n",
    "- `sentence_encoded['input_ids']` is a 2D tensor with shape `(1,N)`, where `N` denotes the number of tokens in the sequence.\n",
    "\n",
    "The `skip_special_tokens=True` parameter is used in the decode method to remove special tokens that were added by the tokenizer. Special tokens: In many NLP models, special tokens are added to the input for specific purposes. Setting `skip_special_tokens=True` tells the tokenizer's decode function to exclude these tokens from the output, so the decoded sentence appears clean and natural without extra symbols.\n",
    "\n",
    "Example: Without `skip_special_tokens=True`, you might see output like this:\n",
    "\n",
    "`How is it going Fakhreddin?</s>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green\"> Model Performance </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_shot_prompt(dialogue):\n",
    "    return dialogue # no prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "Example 0\n",
      "\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# has a dance with Brian at Brian's birthday party. Brian thinks #Person1# looks great and is popular.\n",
      "\n",
      "MODEL GENERATION:\n",
      "Brian, thank you for coming to our party.\n",
      "\n",
      "inputs['input_ids'].size() = torch.Size([1, 197])\n",
      "\n",
      "model_tokenized_output.size() = torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "testdata = dataset['test']\n",
    "test_indices = [11] # working with one single example\n",
    "\n",
    "def generate(make_zero_shot_prompt):\n",
    "    for i, index in enumerate(test_indices):\n",
    "        dialogue = testdata[index]['dialogue']\n",
    "        summary  = testdata[index]['summary']\n",
    "    \n",
    "        prompt = make_zero_shot_prompt(dialogue)\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "        model_tokenized_output = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
    "        output = tokenizer.decode(model_tokenized_output[0], skip_special_tokens=True)\n",
    "    \n",
    "        print(hbar)\n",
    "        print(f\"Example {i}\")\n",
    "        print('\\nINPUT DIALOGUE:')\n",
    "        print(testdata[index]['dialogue'])\n",
    "        print('\\nBASELINE HUMAN SUMMARY:')\n",
    "        print(testdata[index]['summary'])\n",
    "        print('\\nMODEL GENERATION:')\n",
    "        print(output)\n",
    "        print(f\"\\ninputs['input_ids'].size() = {inputs['input_ids'].size()}\")\n",
    "        print(f\"\\nmodel_tokenized_output.size() = {model_tokenized_output.size()}\")\n",
    "        \n",
    "generate(make_zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `model.generate(encoded_sequence['input_ids'])` generates a sequence of tokens for completion. The `encoded_sequence['input_ids']` has a shape (1,N) and the output of this method has shape (1,M). Here, N and M are the number of tokens in the propmt and answer, respectively. `tokenizer.decode` however accepts 1D tokenized seqeuence, so we use `model_tokenized_output[0]` to extract the sequence for decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot with Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_shot_prompt(dialogue):\n",
    "    prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "Example 0\n",
      "\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# has a dance with Brian at Brian's birthday party. Brian thinks #Person1# looks great and is popular.\n",
      "\n",
      "MODEL GENERATION:\n",
      "#Person1#: Happy birthday, Brian. #Person2#: Thank you for coming.\n",
      "\n",
      "inputs['input_ids'].size() = torch.Size([1, 206])\n",
      "\n",
      "model_tokenized_output.size() = torch.Size([1, 24])\n"
     ]
    }
   ],
   "source": [
    "generate(make_zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot and Few-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function takes a list of `full_example_indices`, generates a prompt with full examples;\n",
    "# then at the end appends the prompt which you want the model to complete (`dialogue_to_be_summarized`)\n",
    "def make_few_shot_prompt(full_example_indices, dialogue_to_be_summarized):\n",
    "    prompt = ''\n",
    "    for index in full_example_indices:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"Dialogue:\\n\\n{dialogue}\\n\\nWhat was going on?\\n{summary}\\n\"    \n",
    "    \n",
    "    \n",
    "    prompt += f\"Dialogue:\\n\\n{dialogue_to_be_summarized}\\n\\nWhat was going on?\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Good coming. What can I do for you?\n",
      "#Person2#: I'm in Room 309. I'm checking out today. Can I have my bill now?\n",
      "#Person1#: Certainly. Please wait a moment. Here you are.\n",
      "#Person2#: Thanks. Wait... What's this? The 30 dollar for?\n",
      "#Person1#: Excuse me... The charge for your laundry service on Nov. 20th.\n",
      "#Person2#: But I did't take any laundry service during my stay here. I think you have added someone else's.\n",
      "#Person1#: Ummmm...Sorry, would you mind waiting a moment? We check it with the department concerned.\n",
      "#Person2#: No. As long as we get this straightened out.\n",
      "#Person1#: I'm very sorry. There has been a mistake. We'll correct the bill. Please take a look.\n",
      "#Person2#: Okay, here you are.\n",
      "#Person1#: Goodbye.\n",
      "\n",
      "What was going on?\n",
      "#Person2# finds #Person2# being mischarged. #Person1# corrects the bill and #Person2# pays for it.\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "What was going on?\n",
      "________________________________________\n",
      "Example 0\n",
      "\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# has a dance with Brian at Brian's birthday party. Brian thinks #Person1# looks great and is popular.\n",
      "\n",
      "MODEL GENERATION:\n",
      "Brian's birthday is coming up. Brian's friends are coming to the party.\n",
      "\n",
      "inputs['input_ids'].size() = torch.Size([1, 484])\n",
      "\n",
      "model_tokenized_output.size() = torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "index = test_indices[0]\n",
    "dialogue = testdata[index]['dialogue']\n",
    "summary  = testdata[index]['summary']\n",
    "full_example_indices = [23] # what happens if you use the index of the dialogue_to_be_summarized here?\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "def generate():\n",
    "    prompt = make_few_shot_prompt(full_example_indices, dialogue)\n",
    "    print(f\"prompt:\\n\\n{prompt}\")\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    model_tokenized_output = model.generate(inputs[\"input_ids\"], generation_config=generation_config)\n",
    "    output = tokenizer.decode(model_tokenized_output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(hbar)\n",
    "    print(f\"Example {i}\")\n",
    "    print('\\nINPUT DIALOGUE:')\n",
    "    print(testdata[index]['dialogue'])\n",
    "    print('\\nBASELINE HUMAN SUMMARY:')\n",
    "    print(testdata[index]['summary'])\n",
    "    print('\\nMODEL GENERATION:')\n",
    "    print(output)\n",
    "    print(f\"\\ninputs['input_ids'].size() = {inputs['input_ids'].size()}\")\n",
    "    print(f\"\\nmodel_tokenized_output.size() = {model_tokenized_output.size()}\")\n",
    "    \n",
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Generative Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Good coming. What can I do for you?\n",
      "#Person2#: I'm in Room 309. I'm checking out today. Can I have my bill now?\n",
      "#Person1#: Certainly. Please wait a moment. Here you are.\n",
      "#Person2#: Thanks. Wait... What's this? The 30 dollar for?\n",
      "#Person1#: Excuse me... The charge for your laundry service on Nov. 20th.\n",
      "#Person2#: But I did't take any laundry service during my stay here. I think you have added someone else's.\n",
      "#Person1#: Ummmm...Sorry, would you mind waiting a moment? We check it with the department concerned.\n",
      "#Person2#: No. As long as we get this straightened out.\n",
      "#Person1#: I'm very sorry. There has been a mistake. We'll correct the bill. Please take a look.\n",
      "#Person2#: Okay, here you are.\n",
      "#Person1#: Goodbye.\n",
      "\n",
      "What was going on?\n",
      "#Person2# finds #Person2# being mischarged. #Person1# corrects the bill and #Person2# pays for it.\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "What was going on?\n",
      "________________________________________\n",
      "Example 0\n",
      "\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# has a dance with Brian at Brian's birthday party. Brian thinks #Person1# looks great and is popular.\n",
      "\n",
      "MODEL GENERATION:\n",
      "Brian's birthday is coming up. Brian wants to have a dance with Brian.\n",
      "\n",
      "inputs['input_ids'].size() = torch.Size([1, 484])\n",
      "\n",
      "model_tokenized_output.size() = torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
